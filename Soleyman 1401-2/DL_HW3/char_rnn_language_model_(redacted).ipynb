{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Name:\n",
        "\n",
        "Student ID:"
      ],
      "metadata": {
        "id": "QaJN9rqQBQlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise, you should develop a character-level RNN language model.\n",
        "\n",
        "You are free to choose the architecture, but you must use GRUs and not LSTMs. A linear embedding layer (hidden size 64), a 2-layer GRU (hidden size 128, dropout 0.1), and a linear classifier head is an example architecture.\n",
        "\n",
        "You should generate some example outputs using beam search.\n",
        "\n",
        "Some parts of the code has been done for you. You need to implement the parts that raise `NotImplementedError`.\n",
        "\n",
        "The index zero has been reserved for the padding token/character. By subtracting one from the token indices, the indices will become ASCII indices. (And the padding index will become `-1`.)\n",
        "\n",
        "The model's classification head should directly predict ASCII characters (256 possibilities). It should not predict any special tokens, such as padding, start or end."
      ],
      "metadata": {
        "id": "NREc5PRBBUIq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlQemG3WM9uc"
      },
      "source": [
        "# Bootstrap"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install"
      ],
      "metadata": {
        "id": "cQu2ZQgpN8fK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U torch datasets pyperclip icecream numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjzJdHsqN9Y3",
        "outputId": "df70dac7-267f-4244-a8fb-dc140e650fed"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.5)\n",
            "Requirement already satisfied: pyperclip in /usr/local/lib/python3.10/dist-packages (1.8.2)\n",
            "Requirement already satisfied: icecream in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: colorama>=0.3.9 in /usr/local/lib/python3.10/dist-packages (from icecream) (0.4.6)\n",
            "Requirement already satisfied: pygments>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from icecream) (2.16.1)\n",
            "Requirement already satisfied: executing>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from icecream) (1.2.0)\n",
            "Requirement already satisfied: asttokens>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from icecream) (2.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from asttokens>=2.0.1->icecream) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the Data"
      ],
      "metadata": {
        "id": "Slipo3ggNC1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://files.lilf.ir/Black%20Luminary.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhGFkv0UNFVi",
        "outputId": "3d29ea2b-b8be-4497-d223-53f426efe7e5"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-23 16:45:53--  https://files.lilf.ir/Black%20Luminary.txt\n",
            "Resolving files.lilf.ir (files.lilf.ir)... 82.102.11.148\n",
            "Connecting to files.lilf.ir (files.lilf.ir)|82.102.11.148|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3148450 (3.0M) [text/plain]\n",
            "Saving to: ‘Black Luminary.txt.1’\n",
            "\n",
            "Black Luminary.txt. 100%[===================>]   3.00M  2.98MB/s    in 1.0s    \n",
            "\n",
            "2023-09-23 16:45:55 (2.98 MB/s) - ‘Black Luminary.txt.1’ saved [3148450/3148450]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls -lh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3F23BhCNuOz",
        "outputId": "7dcabcd5-1d42-4661-885a-1db5f86844e6"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 6.1M\n",
            "-rw-r--r-- 1 root root 3.1M Oct 14  2021 'Black Luminary.txt'\n",
            "-rw-r--r-- 1 root root 3.1M Oct 14  2021 'Black Luminary.txt.1'\n",
            "drwxr-xr-x 1 root root 4.0K Sep 21 13:49  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! realpath *.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNcrnYGXN0LL",
        "outputId": "827a8f29-456f-4e47-c085-6d8d2311c87a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Black Luminary.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZsjI90wM9ue"
      },
      "source": [
        "# User Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "fIqRnCu6M9uf"
      },
      "outputs": [],
      "source": [
        "data_paths = [\n",
        "    '/content/Black Luminary.txt',\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Olgn0qCM9ug"
      },
      "source": [
        "## imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "_T0kGW6XM9ug"
      },
      "outputs": [],
      "source": [
        "import pyperclip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "UACStCrvM9uh"
      },
      "outputs": [],
      "source": [
        "from icecream import ic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "9k2815dZM9uh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "d99MwF2iM9uh"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cpu\")\n",
        "#: We will set device again in the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "CurNhMp1M9uh"
      },
      "outputs": [],
      "source": [
        "import datasets as D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "PdwZwv1PM9uh"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "np = numpy\n",
        "\n",
        "import statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuFMb2_pM9ui"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "Ugsrq8zUM9ui"
      },
      "outputs": [],
      "source": [
        "class NumpyPrintOptions:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.options = kwargs\n",
        "        self.original_options = np.get_printoptions()\n",
        "\n",
        "    def __enter__(self):\n",
        "        np.set_printoptions(**self.options)\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        np.set_printoptions(**self.original_options)\n",
        "\n",
        "class NoTruncationNumpyPrintOptions(NumpyPrintOptions):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            threshold=np.inf,\n",
        "            linewidth=200,\n",
        "            suppress=True,\n",
        "            precision=4\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "PpX85hK8M9ui"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "\n",
        "def torch_shape_get(input):\n",
        "    def h_shape_get(x):\n",
        "        return x.dtype, x.shape\n",
        "\n",
        "    return jax.tree_map(h_shape_get, input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "9vTacuN8M9ui"
      },
      "outputs": [],
      "source": [
        "def has_nan(tensor):\n",
        "    return torch.any(torch.isnan(tensor))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelEvalMode:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.model.eval()\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.model.train()"
      ],
      "metadata": {
        "id": "TZGkQ8S9XLsH"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeoLLv_9M9ui"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q44PnV-HM9uj",
        "outputId": "9f6df5af-df92-4fa7-a7b5-167e7ac6fbcb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 18423\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "d = D.load_dataset(\"text\",\n",
        "                         data_files=data_paths, sample_by=\"paragraph\")\n",
        "d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNpSYGHMM9uj",
        "outputId": "25692604-8cad-41ef-e3fe-460e2cc95eaa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text'],\n",
              "    num_rows: 18423\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "d = d['train']\n",
        "d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-TL7zW1M9uj",
        "outputId": "9af9159b-8b84-4397-b786-e4f6ee46616a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': ['Professor Snape threw him backwards, and Harry stumbled, but just managed to keep standing.',\n",
              "  \"'I understand, sir. I apologise if my careless words have offended.'\",\n",
              "  \"'Get going then!' The man turned around and marched towards his desk.\",\n",
              "  'Not keen on his company for the moment, Harry hastily made his way towards the hall.',\n",
              "  \"Merlin, did he have to grab my arm like that? If being implicated in murder is child's play to him, then I can indeed do without partaking in his 'problems'…\",\n",
              "  '~BLHD~',\n",
              "  \"Harry paused before entering the Great Hall and forced his countenance as hard as he could into a blank expression. He must not let up; he must not relent for one second. Weakness would not help him here. It might also be prudent to distance himself a bit from other people for a while to limit the damage to their reputation and family; depending on how the whole situation turned out, the political fallout could be immense. With a sense of foreboding, he imagined Daphne's reaction to his decision, but there really was no choice in the matter. It really was much smarter this way, surely she would see the logic of this…\",\n",
              "  'Carefully arranging his face into an expression of supreme indifference, he opened the door and strode towards the Slytherin table.',\n",
              "  'The hall was eerily silent, and the students sat in groups, fiercely whispering about the latest news, no doubt. The headmaster and most of the staff were absent, but Harry spotted some Aurors discreetly standing in the corners in their stead. As soon as he had entered, all the eyes in the hall had turned towards him, and the angry muttering rose to alarming levels. It was especially bad when he passed the Gryffindor table.',\n",
              "  \"'Can't believe how smug he looks. I guess I don't really mind the Darkers finishing each other off, but look how he doesn't even care that he murdered another student on his first day of school. Freaking psychopath, that one is…'\"]}"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "d[1000:1010]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgR8LYYYM9uk",
        "outputId": "25188e1d-ac05-4a23-a0b5-653ac9be951a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([104, 101, 108, 108, 111], dtype=int8)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "def str_to_np(s, dtype=np.int8):\n",
        "    s = s.encode('ascii', errors='ignore')\n",
        "    return np.frombuffer(s, dtype=dtype)\n",
        "\n",
        "str_to_np('hello')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "f3i9UnniM9uk"
      },
      "outputs": [],
      "source": [
        "def str_to_onehot(s):\n",
        "    return np.eye(256)[str_to_np(s)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6Z9332lM9ul",
        "outputId": "d5db259d-dbc4-4753-e175-e0a8e0586d36"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'input'],\n",
              "    num_rows: 18423\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "dc = d.map(lambda batch: {'input': [str_to_np(t).astype(np.int32) + 1 for t in batch['text']]}, batched=True) #: added one to the char indices to make zero available for the pad token\n",
        "dc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t064BxuIM9ul",
        "outputId": "f81ab4e6-7d12-49c5-cbc3-85b957c2c6c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'input'],\n",
              "    num_rows: 16371\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "dc = dc.filter(lambda x: (len(x['input']) > 30 and len(x['text'].split()) > 4), batched=False)\n",
        "dc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "XuhPgc3YM9ul"
      },
      "outputs": [],
      "source": [
        "dc.set_format(\"torch\", columns=[\"input\",])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxAkoyZrM9ul",
        "outputId": "085e51a3-a43c-4293-874e-f1d111ef148a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(torch.int64, torch.Size([1126])),\n",
              " (torch.int64, torch.Size([727])),\n",
              " (torch.int64, torch.Size([163])),\n",
              " (torch.int64, torch.Size([232])),\n",
              " (torch.int64, torch.Size([106])),\n",
              " (torch.int64, torch.Size([88])),\n",
              " (torch.int64, torch.Size([82])),\n",
              " (torch.int64, torch.Size([69])),\n",
              " (torch.int64, torch.Size([127])),\n",
              " (torch.int64, torch.Size([64]))]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "torch_shape_get(dc[1000:1010]['input'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "DddvvsVKM9ul"
      },
      "outputs": [],
      "source": [
        "dc = dc.shuffle()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl6AuH1pM9ul",
        "outputId": "29acc43e-793a-48af-af60-7e2221a81a80"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'input'],\n",
              "        num_rows: 13096\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'input'],\n",
              "        num_rows: 3275\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "dcs = dc.train_test_split(test_size=0.2)\n",
        "dcs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvXAwGmgM9um"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXpLKcRnM9um"
      },
      "source": [
        "- [GRU --- PyTorch 2.0 documentation](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html)\n",
        "\n",
        "- [torch.nn.utils.rnn.pack_sequence --- PyTorch 2.0 documentation](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_sequence.html#torch.nn.utils.rnn.pack_sequence) (not necessarily needed)\n",
        "\n",
        "- [Embedding --- PyTorch 2.0 documentation](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
        "\n",
        "- [torch.nn.utils.rnn.pad_sequence --- PyTorch 2.0 documentation](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "ggX6CFX1M9um"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size=256, num_layers=1):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Define the layers of your model\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        #: x: list of tensors shaped (seq_length, 256)\n",
        "        #: The seq_length will not necessarily be equal for all list items!\n",
        "        ##\n",
        "        # Assume x is a list of tensors with varying sequence lengths\n",
        "        # First, pad the sequences to make them equal length within the batch\n",
        "        # You can use a utility function or padding within your DataLoader\n",
        "\n",
        "        # Convert the list of tensors to a padded batch tensor\n",
        "        x_padded = torch.nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
        "\n",
        "        # Pass the padded batch through the RNN\n",
        "        out, hidden = self.gru(x_padded, hidden)\n",
        "\n",
        "        # You can use the final hidden state or apply pooling/attention as needed\n",
        "        out = self.fc(out[:, -1, :])  # Get the output at the last time step\n",
        "\n",
        "\n",
        "        return x, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "mhFz1h8PM9um"
      },
      "outputs": [],
      "source": [
        "def loss_fn(y, y_hat):\n",
        "    #: y: list of tensors shaped (seq_length, classes_n+1)\n",
        "    #: y_hat: tensor shaped (B, T, classes_n)\n",
        "    #:\n",
        "    #: Be sure to skip padding!\n",
        "\n",
        "    # Initialize an empty list to store losses for each sequence\n",
        "    sequence_losses = []\n",
        "\n",
        "    # Iterate through each sequence in y\n",
        "    for true_sequence in y:\n",
        "        # Calculate the length of the sequence (excluding padding)\n",
        "        seq_length = len(true_sequence)\n",
        "\n",
        "        # Extract the corresponding predicted sequence (excluding padding)\n",
        "        pred_sequence = y_hat[:seq_length]\n",
        "\n",
        "        # Calculate the Cross-Entropy Loss for this sequence\n",
        "        loss = F.cross_entropy(pred_sequence, true_sequence, ignore_index=-1)\n",
        "\n",
        "        # Append the loss to the list of sequence losses\n",
        "        sequence_losses.append(loss)\n",
        "\n",
        "    # Calculate the average loss over all sequences\n",
        "    average_loss = torch.mean(torch.stack(sequence_losses))\n",
        "\n",
        "    return average_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn3SvgBJM9um",
        "outputId": "bdfa528e-5be6-47d0-853f-0a75cea4713d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Ids:\n",
            "[tensor([1, 2, 3, 4]), tensor([5, 6, 7, 8])]\n",
            "Shifted Left (Target Ids):\n",
            "[tensor([2, 3, 4, 0]), tensor([6, 7, 8, 0])]\n"
          ]
        }
      ],
      "source": [
        "def shift_left(tensor_list, pad_value=0):\n",
        "    shifted_tensors = []\n",
        "    for tensor in tensor_list:\n",
        "        # Check if the tensor is empty or has only one element\n",
        "        if tensor.size(0) <= 1:\n",
        "            # If so, add a new tensor with a single element containing the pad_value\n",
        "            shifted_tensors.append(torch.tensor([pad_value]))\n",
        "        else:\n",
        "            # Otherwise, remove the first element and pad at the end\n",
        "            shifted_tensor = torch.cat((tensor[1:], torch.tensor([pad_value])), dim=0)\n",
        "\n",
        "            shifted_tensors.append(shifted_tensor)\n",
        "    return shifted_tensors\n",
        "\n",
        "# Example usage:\n",
        "input_ids = [torch.tensor([1, 2, 3, 4]), torch.tensor([5, 6, 7, 8])]\n",
        "print(\"Input Ids:\")\n",
        "print(input_ids)\n",
        "\n",
        "target_ids = shift_left(input_ids)\n",
        "print(\"Shifted Left (Target Ids):\")\n",
        "print(target_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beam Search Generation"
      ],
      "metadata": {
        "id": "2CnBxRPREInk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "V4SbAc1JM9um"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import heapq\n",
        "\n",
        "def tensor_to_string(tensor):\n",
        "    chars = [chr(c) for c in tensor]\n",
        "    return ''.join(chars)\n",
        "\n",
        "def tensor_append_scalar(tensor, scalar):\n",
        "    scalar_tensor = torch.tensor(scalar).view(1)  # Add a dimension to match the original tensor's dimensions\n",
        "    scalar_tensor = scalar_tensor.to(device)\n",
        "\n",
        "    # Append the scalar to the original tensor\n",
        "    result = torch.cat((tensor, scalar_tensor), dim=0)\n",
        "    return result\n",
        "\n",
        "\n",
        "def generate_next_top_k(model, input_sequence, k):\n",
        "    logits, _ = model.forward([input_sequence])\n",
        "    logits = logits[0, -1, :]\n",
        "    # ic(torch_shape_get(logits))\n",
        "\n",
        "    probabilities = torch.softmax(logits, dim=-1)\n",
        "    # ic(torch_shape_get(probabilities))\n",
        "\n",
        "    top_k_values, top_k_indices = torch.topk(probabilities, k)\n",
        "\n",
        "    return [(tensor_append_scalar(input_sequence, idx.item() + 1), log_prob.item()) for idx, log_prob in zip(top_k_indices, top_k_values.log())]\n",
        "\n",
        "def beam_search(model, desired_length, starting_string, k=5):\n",
        "    with ModelEvalMode(model), torch.no_grad():\n",
        "      input_sequence = torch.tensor(str_to_np(starting_string).astype(np.int32) + 1, dtype=torch.long)\n",
        "      input_sequence = input_sequence.to(device)\n",
        "      # ic(torch_shape_get(input_sequence))\n",
        "\n",
        "      log_prob = 0.0\n",
        "\n",
        "      beam = [(input_sequence, log_prob)]\n",
        "\n",
        "      while len(beam[0][0]) < desired_length:\n",
        "          new_beam = []\n",
        "          for seq, log_prob in beam:\n",
        "              next_top_k = generate_next_top_k(model, seq, k)\n",
        "              new_beam.extend([(new_seq, new_log_prob + log_prob) for new_seq, new_log_prob in next_top_k])\n",
        "\n",
        "          beam = heapq.nlargest(k, new_beam, key=lambda x: x[1])\n",
        "\n",
        "      return [tensor_to_string(seq - 1) for seq, _ in beam]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "7YZFXxwtM9un",
        "outputId": "cf0af304-45de-4890-d46d-9b1d56ad4e26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ic| a: array([ 73,  98, 115, 115, 122,  33], dtype=int32)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Harry '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "a = str_to_np(\"Harry \").astype(np.int32) + 1\n",
        "ic(a)\n",
        "tensor_to_string(a - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "sf6P5sQZM9un"
      },
      "outputs": [],
      "source": [
        "def eval_gen(*args, display=999999, **kwargs):\n",
        "    generated_texts = beam_search(\n",
        "        *args, **kwargs,\n",
        "    )\n",
        "\n",
        "    for idx, text in enumerate(generated_texts):\n",
        "        if idx >= display:\n",
        "            break\n",
        "\n",
        "        print(f\"Generated text {idx + 1}: {text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "SekVnAUcOuiU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWAhPH-UM9um",
        "outputId": "52f32062-e8b3-4fac-8dde-06ead38e9a03"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'input'],\n",
              "    num_rows: 13096\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "dt = dcs['train']\n",
        "dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "1YSVRWabM9un",
        "outputId": "2264ffc7-2a0f-40f3-897e-740d5da8a1d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-347e3eed0e4a>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                                 \u001b[0;31m# Replace NotImplementedError() with your model's forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Unsqueeze to add batch dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                                 \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-73-6cea247d9eeb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Pass the padded batch through the RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# You can use the final hidden state or apply pooling/attention as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    997\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    216\u001b[0m                     expected_input_dim, input.dim()))\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    219\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[1;32m    220\u001b[0m                     self.input_size, input.size(-1)))\n",
            "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 256, got 62"
          ]
        }
      ],
      "source": [
        "#: Training Loop\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "\tdevice = 'cuda'\n",
        "\tnon_blocking = True\n",
        "elif True:\n",
        "\tdevice = 'cpu'\n",
        "\tnon_blocking = False\n",
        "else:\n",
        "\t#: causes NaNs\n",
        "\tdevice = 'mps'\n",
        "\tnon_blocking = False\n",
        "\n",
        "i = 0\n",
        "\n",
        "#: Feel free to edit these hyperparameters or the optimizer\n",
        "#: You might want to use a learning-rate scheduler, such as\n",
        "#: [ReduceLROnPlateau --- PyTorch 2.0 documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html)\n",
        "epochs = 400\n",
        "batch_size = 4096\n",
        "learning_rate = 0.01\n",
        "max_len = 0\n",
        "\n",
        "# Define the input and output sizes based on your data\n",
        "input_size = 256  # Adjust this to match your input size\n",
        "output_size = 10  # Adjust this to match your output size\n",
        "\n",
        "# Create an instance of your model with the specified input and output sizes\n",
        "m = Model(input_size, output_size).to(device=device, non_blocking=non_blocking)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "\n",
        "counter = 0\n",
        "for epoch in range(epochs):\n",
        "\n",
        "\tm.train()\n",
        "\tdt = dt.shuffle()\n",
        "\n",
        "\tfor i in range(0, len(dt), batch_size):\n",
        "\t\tbatch = dt[i:i+batch_size]\n",
        "\t\tinputs = batch['input']\n",
        "\n",
        "\t\tif max_len > 0:\n",
        "\t\t\tinputs = list(map(lambda x: x[:max_len] if len(x) > max_len else x, inputs))\n",
        "\n",
        "\t\tlens = [len(seq) for seq in inputs]\n",
        "\t\tcurrent_max_len = max(lens)\n",
        "\t\tmean_len = statistics.mean(lens)\n",
        "\t\t# ic(current_max_len, mean_len)\n",
        "\n",
        "\t\tinputs = list(map(lambda x: x.to(device, non_blocking=non_blocking), inputs))\n",
        "\t\t# ic(torch_shape_get(inputs))\n",
        "\n",
        "\t\ttargets = shift_left(inputs)\n",
        "\t\t# ic(torch_shape_get(targets))\n",
        "\n",
        "\t\ttotal_loss = 0\n",
        "\n",
        "\t\toptimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "\t\t# Forward pass\n",
        "\t\tfor seq, target in zip(inputs, targets):\n",
        "\t\t\t\t# Replace NotImplementedError() with your model's forward pass\n",
        "\t\t\t\toutput, _ = m(seq.unsqueeze(0))  # Unsqueeze to add batch dimension\n",
        "\t\t\t\tloss = criterion(output.view(-1, output.size(-1)), target.view(-1))\n",
        "\t\t\t\ttotal_loss += loss\n",
        "\n",
        "\t\t# Backpropagation\n",
        "\t\ttotal_loss.backward()\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\t\tl = total_loss / mean_len\n",
        "\n",
        "\t\tif counter % 1 == 0:\n",
        "\t\t\tl = l.item()\n",
        "\t\t\tprint(f\"loss: {l:>7f}  [{counter:>5d}, epoch={epoch}]\")\n",
        "\n",
        "\t\tcounter += 1\n",
        "\n",
        "\t# print(f\"loss: {l:>7f}  [{counter-1:>5d}, epoch={epoch} finished!]\")\n",
        "\tif epoch % 15 == 0:\n",
        "\t\teval_gen(display=3, model=m, desired_length=100, starting_string=\"Harry \", k=32)\n",
        "\n",
        "# Optionally, save the trained model\n",
        "torch.save(m.state_dict(), 'char_rnn_language_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JTIoWRkM9un"
      },
      "outputs": [],
      "source": [
        "eval_gen(display=3, model=m, desired_length=100, starting_string=\"Harry \", k=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecXrWcqLM9uo"
      },
      "outputs": [],
      "source": [
        "eval_gen(display=50, model=m, desired_length=250, starting_string=\"Harry \", k=100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_gen(display=50, model=m, desired_length=250, starting_string=\"Arcturus \", k=100)"
      ],
      "metadata": {
        "id": "3r-PhNIMGeDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAXUB8vvM9un"
      },
      "outputs": [],
      "source": [
        "eval_gen(display=50, model=m, desired_length=150, starting_string=\"Draco \", k=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QKfmToSM9uo"
      },
      "outputs": [],
      "source": [
        "eval_gen(display=50, model=m, desired_length=150, starting_string=\"Harry looked at \", k=100)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cQu2ZQgpN8fK",
        "Slipo3ggNC1I",
        "NZsjI90wM9ue",
        "7Olgn0qCM9ug",
        "fuFMb2_pM9ui"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}