{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4_q3YCOjUlc"
      },
      "outputs": [],
      "source": [
        " import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        " \n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tqdm.notebook import tqdm \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HC4GVy8Osn6"
      },
      "source": [
        "In this homework, we will explore deep networks using  PyTorch framework!\n",
        "With the MNIST dataset consisting of 70000 images (28 * 28 * 1) of handwritten digits, we will train a multiple-layer perceptron network to detect the digit corresponding to the input image. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zeodqY-UL9t"
      },
      "source": [
        "#Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NIB1N86T4vP"
      },
      "source": [
        "Run the cell below to download the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JE6t9NG3XcnB"
      },
      "outputs": [],
      "source": [
        "\n",
        "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "mnist_test = datasets.MNIST(root='./data', train=False, download=True,transform=transforms.ToTensor()) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfqms4_IjlmL"
      },
      "outputs": [],
      "source": [
        "mnist_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qw8tVtRe5St_"
      },
      "outputs": [],
      "source": [
        "mnist_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSVexjkV6OX5"
      },
      "outputs": [],
      "source": [
        "image, label = mnist_test[0]\n",
        "label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4l05jpAUDdH"
      },
      "outputs": [],
      "source": [
        "image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGYK9gIi6TcS"
      },
      "outputs": [],
      "source": [
        "plt.imshow(image.reshape(28,28), cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5-BCDKAV0RV"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZauc_yJV7l7"
      },
      "source": [
        "\n",
        "The model we want to classify the MNIST dataset is a two-hidden-layer MLP with 120 and 84 units in each layer, respectively. \n",
        "\n",
        "Create a PyTorch model for described model and use ReLU for the activation function.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTOrHU2s7ww6"
      },
      "outputs": [],
      "source": [
        "class MultilayerPerceptron(nn.Module):\n",
        "\n",
        "    #todo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8lVcUidYxZR"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(80)\n",
        "model = MultilayerPerceptron()\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHCtONg8LbYU"
      },
      "source": [
        "Answer these questions:\n",
        "2. Denote the size of the weights matrix of each layer (consider the input and output layer as well). \n",
        "1. What initial values are used  ​​for the weights and biases of your model? (Do not report the specific value, we are interested in distributions that are used.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWv1_hwjkq4j"
      },
      "source": [
        "#Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLbpRcjokxUb"
      },
      "source": [
        "In this section, we want to train our MLP model using the SGD algorithm. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUmhmB0EG7KJ"
      },
      "source": [
        "As we discussed in class, instead of calculating loss using the whole inputs, we break the dataset (with size $n$) into some batches of size m ($m < n$) and, at each iteration, calculate the loss of one batch and update the weights.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XV7HFbxUa-v"
      },
      "source": [
        "Using the `DataLoader` class of the torch library, complete the cell below to batch the test and train dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Drvm3GNL7eTd"
      },
      "outputs": [],
      "source": [
        "batch_size = 100\n",
        "train_loader = #todo\n",
        "test_loader = #todo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-sIe-gSF3VY"
      },
      "source": [
        "there are yet a couple of hyperprameters that we should define before training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_u-Vzy9zURKD"
      },
      "outputs": [],
      "source": [
        "\n",
        "epochs = 10\n",
        "learning_rate = 0.01 #feel free to play with this\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TI9B7UQ1hu8"
      },
      "source": [
        "Answer these questions:\n",
        "* Prove that the **expected value** of the gradient obtained using a random batch equals the gradient of the whole dataset.\n",
        "\n",
        "* For the dataset of size $n = $12000, if the batch size is 200, what would be the number of weight updates (or iterations) in a total of 5 epoche?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRCAmupNPnRK"
      },
      "source": [
        "\n",
        "Complete the training function below, such that at each epoch:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1. For each batch:\n",
        "\n",
        "  1. Perform feedforward\n",
        "  2. Calculate loss (use cross-entropy loss)\n",
        "  3. Perform backpropagation and updating weights\n",
        "\n",
        "2. Calculate and report the training accuracy and loss for each epoch (use the average of the batch accuracy as epoch accuracy).\n",
        "3. Calculate and report the test accuracy and loss for each epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toDAqJnr73Ne"
      },
      "outputs": [],
      "source": [
        "#initialize loss function and optimizer\n",
        "loss_function = #todo\n",
        "optimizer = #todo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6cNtYZS8o72"
      },
      "source": [
        "Highly encourage you to use [this](https://github.com/tqdm/tqdm) awesome package called taqaddum to have a nice progress bar for the training process\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIj4OrwYE7Hn"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(model, loss_function, optimizer, train_loader, test_loader, epochs):\n",
        "\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    train_acc = []\n",
        "    test_acc  = []\n",
        "\n",
        "\n",
        "\n",
        "    #todo\n",
        "\n",
        "\n",
        "\n",
        "    return train_losses, train_acc, test_acc, test_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovj6uYbAtm8L"
      },
      "outputs": [],
      "source": [
        "train_losses, train_acc, test_acc, test_losses = train(model, loss_function, optimizer, train_loader, test_loader, epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BV2NqFdwmUO"
      },
      "source": [
        "Now plot the training results (one plot for test and training accuracy and one for the loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVeoEM7QLHSr"
      },
      "outputs": [],
      "source": [
        "#plot loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tFdIJD-x96y"
      },
      "outputs": [],
      "source": [
        "#plot accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLBY62Fcw4wt"
      },
      "source": [
        "Based on your plots, answer these questions:\n",
        "1. Compare the training and test loss\n",
        "2. Did your model become overfitted? Why?\n",
        "3. Why your training loss is not a  decreasing function with respect to #epoch?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vhuk-mnf2iIn"
      },
      "source": [
        "Repeat the training process for 5 diffrent learning rate and compare the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmMMXJSl2g4h"
      },
      "outputs": [],
      "source": [
        "#to do"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ5vhsvjyIr7"
      },
      "source": [
        "#Vanishing Gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we want to see the vanishing gradient problem.\n",
        "\n",
        "Create two separate models with 5 hidden layers of size [100, 100, 80, 50, 30],\n",
        "one of them uses ReLU as an activation function and the other one uses Sigmoid.\n"
      ],
      "metadata": {
        "id": "BdqbTH68J5hF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltw-XU-Xy8kY"
      },
      "outputs": [],
      "source": [
        "class MLP_relu(nn.Module):\n",
        "\n",
        "  #todo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_sigmoid(nn.Module):\n",
        "\n",
        "  #todo\n"
      ],
      "metadata": {
        "id": "F9IL6wQhpg7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#instantiate the models\n"
      ],
      "metadata": {
        "id": "CJw_AgIqqB0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modify the train function to save the gradient and weights of each layer, every epoch. Return these values as well. "
      ],
      "metadata": {
        "id": "6yn7GJCIxr8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loss_function, optimizer, train_loader, test_loader, epochs):\n",
        "\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    train_acc = []\n",
        "    test_acc  = []\n",
        "    weights = {'H1':[], 'H2':[], 'H3':[],'H4':[], 'H5':[]}\n",
        "    biases = {'H1':[], 'H2':[], 'H3':[],'H4':[], 'H5':[]}\n",
        "    grads = {'H1':[], 'H2':[], 'H3':[],'H4':[], 'H5':[]}\n",
        "\n",
        "\n",
        "    #todo\n",
        "\n",
        "\n",
        "\n",
        "    return train_losses, train_acc, test_acc, test_losses, weights, grads"
      ],
      "metadata": {
        "id": "owVOfUYjPLei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, train the sigmoid and relu model."
      ],
      "metadata": {
        "id": "XLsPDMyiPKYT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpfTlX_dzUeo"
      },
      "outputs": [],
      "source": [
        "#training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oj0CjH_kzbrt"
      },
      "outputs": [],
      "source": [
        "#plot loss\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plot accuracy"
      ],
      "metadata": {
        "id": "6IA1xaS5QkWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the sigmoid model is not good at all. Based on the following plots explain why."
      ],
      "metadata": {
        "id": "qcyJufaazMzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each layer, calculate the mean and std (standard deviation) of all its weights and plot these values ​​vs #epochs. Repeat the same for the grads.\n",
        "\n",
        "You should do:\n",
        "\n",
        "* Calculate the average weight of each hidden layer in each epoch.\n",
        "* Calculate the std for the weight of each hidden layer in each epoch.\n",
        "\n",
        "* Calculate the average grads of each hidden layer in each epoch.\n",
        "* Calculate the std for the grads of each hidden layer in each epoch.\n",
        "\n",
        "Plot all of them for both models (use subplot to compare two models' results).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F3wSt4k5znex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#calcute means and stds"
      ],
      "metadata": {
        "id": "RGXk99Ht10Gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot weights mean for the both models"
      ],
      "metadata": {
        "id": "Vq35DPN41nWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot weights std for the both models"
      ],
      "metadata": {
        "id": "etPX2STS1u9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot grads mean for the both models"
      ],
      "metadata": {
        "id": "zoXOrJYT1xoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot grads std for the both models"
      ],
      "metadata": {
        "id": "tUj9Q65E1x6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Good Luck!"
      ],
      "metadata": {
        "id": "s5-2hPa-5Zww"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}